import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Set the path to your initial dataset folder
initial_dataset_path = "/Users/azra/Downloads/asl_dataset"

# Set the path to your new dataset folder
new_dataset_path = "/Users/azra/Downloads/output/dataset"  # Replace with the actual path

# Define image width and height (resize the images to a uniform size)
image_width, image_height = 100, 100

# Initialize lists to store images and labels
images = []
labels = []

# Load and preprocess images from the initial dataset
for letter in os.listdir(initial_dataset_path):
    letter_path = os.path.join(initial_dataset_path, letter)
    if os.path.isdir(letter_path):
        for filename in os.listdir(letter_path):
            image_path = os.path.join(letter_path, filename)
            image = cv2.imread(image_path)
            if image is not None:
                image = cv2.resize(image, (image_width, image_height))
                print(f"Appending image: {image_path}")
                images.append(image)
                label = ord(letter) - ord('a')  # Assuming uppercase letters
                labels.append(label)
            else:
                print(f"Could not read image: {image_path}")

unique_labels = np.unique(labels)
print("Unique labels:", unique_labels)


# Convert lists to numpy arrays
images = np.array(images)
labels = np.array(labels)
num_classes = 26

# Convert labels to one-hot encoded vectors
y_train = to_categorical(labels, num_classes)

# Normalize pixel values to range [0, 1]
images = images.astype('float32') / 255.0

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(images, y_train, test_size=0.2, random_state=42)

# Define the CNN model architecture
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(image_width, image_height, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model on the initial dataset
model.fit(X_train, y_train, epochs=7, batch_size=32, validation_data=(X_val, y_val))


# Load and preprocess images from the new dataset
new_images = []
new_labels = []


# Load and preprocess images from the initial dataset
for letter in os.listdir(initial_dataset_path):
    if letter == '.DS_Store':
        continue  # Skip the .DS_Store file
    letter_path = os.path.join(initial_dataset_path, letter)
    if os.path.isdir(letter_path):
        for filename in os.listdir(letter_path):
            image_path = os.path.join(letter_path, filename)
            image = cv2.imread(image_path)
            if image is not None:
                image = cv2.resize(image, (image_width, image_height))
                print(f"Appending image: {image_path}")
                new_images.append(image)
                label = ord(letter) - ord('a')  # Assuming uppercase letters
                new_labels.append(label)
            else:
                print(f"Could not read image: {image_path}")


# Convert new_labels to one-hot encoded vectors
new_y_train = to_categorical(new_labels, num_classes)

# Normalize pixel values for the new images
new_images = np.array(new_images).astype('float32') / 255.0

print("Size of new_images:", len(new_images))
print("Size of new_y_train:", len(new_y_train))

# Combine the datasets
X_train_combined = np.concatenate((X_train, new_images), axis=0)
y_train_combined = np.concatenate((y_train, new_y_train), axis=0)
X_val_combined = np.concatenate((X_val, new_images), axis=0)
y_val_combined = np.concatenate((y_val, new_y_train), axis=0)

# Compile the model again (this is optional, you can reuse the previous model)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model on the combined datasets
model.fit(X_train_combined, y_train_combined, epochs=7, batch_size=32, validation_data=(X_val_combined, y_val_combined))

# Save the trained model
model_save_path = "/Users/azra/Desktop/Submission_combined.h5"
model.save(model_save_path)
print(f"Combined model saved to {model_save_path}")
